<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习之神经网络实现MNIST手写字识别"><meta name="keywords" content="Python,Machine-Learning,机器学习,神经网络,MNIST"><meta name="author" content="wnxy"><meta name="copyright" content="wnxy"><title>机器学习之神经网络实现MNIST手写字识别 | Wnxy's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Wnxy's Blog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、神经网络原理"><span class="toc-number">1.</span> <span class="toc-text">一、神经网络原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-模型"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-前向传播"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-激活函数"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.2.1 激活函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-前向传播过程"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2.2 前向传播过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-代价函数"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-反向传播算法"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-推导过程"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.4.1 推导过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、数据集解析"><span class="toc-number">2.</span> <span class="toc-text">二、数据集解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、神经网络搭建"><span class="toc-number">3.</span> <span class="toc-text">三、神经网络搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四、附录"><span class="toc-number">4.</span> <span class="toc-text">四、附录</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">wnxy</div><div class="author-info__description text-center">愿一切美好都能如约而至，喜欢的都能拥有</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">50</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">86</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">10</span></a></div><hr><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;">      </canvas><script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js">     </script><script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js">      </script><script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js">      </script><style>.twopeople{
margin: 0;
align-items: center;
justify-content: center;
text-align: center;
}
canvas {
display: block;
margin: 0 auto;
cursor: move;
}</style></div></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wnxy's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">机器学习之神经网络实现MNIST手写字识别</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-10-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Machine-Learning/">Machine Learning</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一、神经网络原理"><a href="#一、神经网络原理" class="headerlink" title="一、神经网络原理"></a>一、神经网络原理</h2><p>线性回归（Linear Regression）和逻辑回归（Logistic Regression）通常用来处理线性模型，如果利用线性回归或逻辑回归对多特征的非线性问题进行分类，则涉及太多特征组合的计算，往往导致计算负荷增大，并不适合解决这类问题。</p>
<p>假设我们需要训练一个模型用来判断一张图片中是否出现汽车，可能有很多用来训练模型的数据，这些图片有的包含小汽车，有的没有，利用这些图片的一个个像素值作为特征，训练一个满足这样功能的模型。训练过程需要处理可能百万级别甚至更多的数据，对于这样问题通常采用神经网络（Neural Networks）解决。</p>
<a id="more"></a>
<h3 id="1-1-模型"><a href="#1-1-模型" class="headerlink" title="1.1 模型"></a>1.1 模型</h3><p><a href="https://imgchr.com/i/BQBHfO" target="_blank" rel="noopener"><img src="/loading.gif" data-original="https://s1.ax1x.com/2020/10/27/BQBHfO.png" alt=""></a></p>
<p>这个一个简单的3层神经网络，第一层为输入层（Input Layers），最后一层为输出层（Output Layers），中间层称为隐藏层（Hidden Layers）</p>
<h3 id="1-2-前向传播"><a href="#1-2-前向传播" class="headerlink" title="1.2 前向传播"></a>1.2 前向传播</h3><h4 id="1-2-1-激活函数"><a href="#1-2-1-激活函数" class="headerlink" title="1.2.1 激活函数"></a>1.2.1 激活函数</h4><p>（1）Sigmoid函数</p>
<p>一个常见的激活函数，其数学表达式为<a href="https://www.codecogs.com/eqnedit.php?latex=Sig(z)=\frac{1}{1&plus;e^{-z}}" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?Sig(z)=\frac{1}{1&plus;e^{-z}}" title="Sig(z)=\frac{1}{1+e^{-z}}" /></a>。</p>
<p>Python实现代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>
<p>（2）ReLU函数</p>
<p>计算速度更快，是目前的主流激活函数。数学表达式为：<a href="https://www.codecogs.com/eqnedit.php?latex=ReLU(x)=\left\{\begin{matrix}&space;x&space;&&space;if&space;x>0\\&space;0&space;&&space;if&space;x\leq&space;0&space;\end{matrix}\right." target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?ReLU(x)=\left\{\begin{matrix}&space;x&space;&&space;if&space;x>0\\&space;0&space;&&space;if&space;x\leq&space;0&space;\end{matrix}\right." title="ReLU(x)=\left\{\begin{matrix} x & if x>0\\ 0 & if x\leq 0 \end{matrix}\right." /></a></p>
<p>Python实现代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br></pre></td></tr></table></figure>
<h4 id="1-2-2-前向传播过程"><a href="#1-2-2-前向传播过程" class="headerlink" title="1.2.2 前向传播过程"></a>1.2.2 前向传播过程</h4><p>除输入层外，每一层神经元都有前一层的神经元作为本层神经元的输入，本层神经元的输出又可以作为下一层神经元的输入，依次向前传播最终得到一个输出值，这个由输入值经输入层经过一系列处理最终到达输出层得到输出值的过程称为前向传播。</p>
<p><a href="https://imgchr.com/i/B0npqA" target="_blank" rel="noopener"><img src="/loading.gif" data-original="https://s1.ax1x.com/2020/11/01/B0npqA.md.png" alt=""></a></p>
<p>其中x1, x2, x3是输入单元，即原始的输入数据，a1, a2, a3是中间单元，负责将输入的数据处理然后传递到下一层，最后是输出单元，负责计算<a href="https://www.codecogs.com/eqnedit.php?latex=h_{\Theta&space;}(x)" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?h_{\Theta&space;}(x)" title="h_{\Theta }(x)" /></a>。计算的过程中为每一层都添加了一个偏置（bias unit）。</p>
<p>上图中<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{1}" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\Theta&space;_{1}" title="\Theta _{1}" /></a>，<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{2}" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\Theta&space;_{2}" title="\Theta _{2}" /></a>分别代表输入层到隐藏层的权重和隐藏层到输出层的权重，对于上图的网络模型激活单元和输出分别表达为：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=a_{1}^{(2)}=g(\Theta&space;_{10}^{(1)}x_{0}&plus;\Theta&space;_{11}^{(1)}x_{1}&plus;\Theta&space;_{12}^{(1)}x_{2}&plus;\Theta&space;_{13}^{(1)}x_{3})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?a_{1}^{(2)}=g(\Theta&space;_{10}^{(1)}x_{0}&plus;\Theta&space;_{11}^{(1)}x_{1}&plus;\Theta&space;_{12}^{(1)}x_{2}&plus;\Theta&space;_{13}^{(1)}x_{3})" title="a_{1}^{(2)}=g(\Theta _{10}^{(1)}x_{0}+\Theta _{11}^{(1)}x_{1}+\Theta _{12}^{(1)}x_{2}+\Theta _{13}^{(1)}x_{3})" /></a></p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=a_{2}^{(2)}=g(\Theta&space;_{20}^{(1)}x_{0}&plus;\Theta&space;_{21}^{(1)}x_{1}&plus;\Theta&space;_{22}^{(1)}x_{2}&plus;\Theta&space;_{23}^{(1)}x_{3})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?a_{2}^{(2)}=g(\Theta&space;_{20}^{(1)}x_{0}&plus;\Theta&space;_{21}^{(1)}x_{1}&plus;\Theta&space;_{22}^{(1)}x_{2}&plus;\Theta&space;_{23}^{(1)}x_{3})" title="a_{2}^{(2)}=g(\Theta _{20}^{(1)}x_{0}+\Theta _{21}^{(1)}x_{1}+\Theta _{22}^{(1)}x_{2}+\Theta _{23}^{(1)}x_{3})" /></a></p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=a_{3}^{(2)}=g(\Theta&space;_{30}^{(1)}x_{0}&plus;\Theta&space;_{31}^{(1)}x_{1}&plus;\Theta&space;_{32}^{(1)}x_{2}&plus;\Theta&space;_{33}^{(1)}x_{3})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?a_{3}^{(2)}=g(\Theta&space;_{30}^{(1)}x_{0}&plus;\Theta&space;_{31}^{(1)}x_{1}&plus;\Theta&space;_{32}^{(1)}x_{2}&plus;\Theta&space;_{33}^{(1)}x_{3})" title="a_{3}^{(2)}=g(\Theta _{30}^{(1)}x_{0}+\Theta _{31}^{(1)}x_{1}+\Theta _{32}^{(1)}x_{2}+\Theta _{33}^{(1)}x_{3})" /></a></p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=h_{\Theta&space;}(x)=g(\Theta&space;_{10}^{(2)}a_{0}^{(2)}&plus;\Theta&space;_{11}^{(2)}a_{1}^{(2)}&plus;\Theta&space;_{12}^{(2)}a_{2}^{(2)}&plus;\Theta&space;_{13}^{(2)}a_{3}^{(2)})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?h_{\Theta&space;}(x)=g(\Theta&space;_{10}^{(2)}a_{0}^{(2)}&plus;\Theta&space;_{11}^{(2)}a_{1}^{(2)}&plus;\Theta&space;_{12}^{(2)}a_{2}^{(2)}&plus;\Theta&space;_{13}^{(2)}a_{3}^{(2)})" title="h_{\Theta }(x)=g(\Theta _{10}^{(2)}a_{0}^{(2)}+\Theta _{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta _{13}^{(2)}a_{3}^{(2)})" /></a></p>
<p>如此，从左到右的算法称为前向传播算法，实际应用中为了计算方便通常是以矩阵方式计算的。</p>
<p>Python实现示例（示例不能直接运行）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,X)</span>:</span></span><br><span class="line">    <span class="comment"># 前向传播算法forward_propagation，实现预测</span></span><br><span class="line">    self.a1 = X.T</span><br><span class="line">    self.z2 = np.dot(self.W1,self.a1)+self.b1</span><br><span class="line">    self.a2 = relu(self.z2)</span><br><span class="line">    self.z3 = np.dot(self.W2,self.a2)+self.b2</span><br><span class="line">    self.a3 = relu(self.z3)</span><br><span class="line">    out = self.a3</span><br><span class="line">    p = np.argmax(out, axis=<span class="number">0</span>)  <span class="comment"># 输出层的最大索引下标即为标签值，标签值0-9</span></span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<h3 id="1-3-代价函数"><a href="#1-3-代价函数" class="headerlink" title="1.3 代价函数"></a>1.3 代价函数</h3><p><a href="https://imgchr.com/i/B0M4F1" target="_blank" rel="noopener"><img src="/loading.gif" data-original="https://s1.ax1x.com/2020/11/01/B0M4F1.md.png" alt=""></a></p>
<p>通过代价函数观察预测的结果与真实情况的误差有多大。</p>
<h3 id="1-4-反向传播算法"><a href="#1-4-反向传播算法" class="headerlink" title="1.4 反向传播算法"></a>1.4 反向传播算法</h3><p>一般的训练算法可以分为两个阶段：</p>
<p>（1）求解代价函数关于权值（参数）的导数。（BP）</p>
<p>（2）用得到的导数进一步计算权值的调整量。（梯度下降等优化算法）</p>
<p>反向传播（BP）算法主要应用第一阶段,非常高效的计算这些导数。</p>
<h4 id="1-4-1-推导过程"><a href="#1-4-1-推导过程" class="headerlink" title="1.4.1 推导过程"></a>1.4.1 推导过程</h4><p>假设有一个四层的神经网络，其相关参数为： Sl=4, L=4（其中L表示网络层数，Sl表示l层有多少个神经元），用<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a>表示误差，结合前面介绍的前向传播过程，则：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\sigma&space;^{(4)}=a^{(4)}-y" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\sigma&space;^{(4)}=a^{(4)}-y" title="\sigma ^{(4)}=a^{(4)}-y" /></a></p>
<p>前一层误差为：<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma&space;^{(3)}=(\Theta&space;^{(3)})^{T}\sigma&space;^{(4)}*g^{'}(z^{(2)})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\sigma&space;^{(3)}=(\Theta&space;^{(3)})^{T}\sigma&space;^{(4)}*g^{'}(z^{(2)})" title="\sigma ^{(3)}=(\Theta ^{(3)})^{T}\sigma ^{(4)}*g^{'}(z^{(3)})" /></a></p>
<p>其中<a href="https://www.codecogs.com/eqnedit.php?latex=g^{'}(z^{(2)})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?g^{'}(z^{(2)})" title="g^{'}(z^{(2)})" /></a>是Sig函数的导数。</p>
<p>接着第二层的误差为：<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma&space;^{(2)}=(\Theta&space;^{(2)})^{T}\sigma&space;^{(3)}*g^{'}(z^{(2)})" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\sigma&space;^{(2)}=(\Theta&space;^{(2)})^{T}\sigma&space;^{(3)}*g^{'}(z^{(2)})" title="\sigma ^{(2)}=(\Theta ^{(2)})^{T}\sigma ^{(3)}*g^{'}(z^{(2)})" /></a></p>
<p>第一层是输入变量，不存在误差，此时不考虑正则项，则有：<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;}{\partial&space;\Theta&space;_{ij}^{(l)}}J(\Theta&space;)=a_{ij}^{(l)}\sigma&space;_{i}^{l&plus;1}" target="_blank"><img src="/loading.gif" data-original="https://latex.codecogs.com/gif.latex?\frac{\partial&space;}{\partial&space;\Theta&space;_{ij}^{(l)}}J(\Theta&space;)=a_{ij}^{(l)}\sigma&space;_{i}^{l&plus;1}" title="\frac{\partial }{\partial \Theta _{ij}^{(l)}}J(\Theta )=a_{ij}^{(l)}\sigma _{i}^{l+1}" /></a></p>
<p>要求的导数 = 权值输出端单元的误差项 * 权值输入端单元的激活值。</p>
<p>Python实现示例（示例不能直接运行）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#反向传播：</span></span><br><span class="line"><span class="comment">#1、m表示样本个数</span></span><br><span class="line"><span class="comment">#2、梯度下降，更新W和b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dAL)</span>:</span></span><br><span class="line">    m=<span class="number">60000</span></span><br><span class="line">    dZ3=np.multiply(dAL,relu_derivative(self.z3))</span><br><span class="line">    dW2=np.dot(dZ3, self.a2.T)/m</span><br><span class="line">    db2=np.mean(dZ3,axis=<span class="number">1</span>)</span><br><span class="line">    dAL_1 = np.dot(self.W2.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dAL_1, relu_derivative(self.z2))</span><br><span class="line">    dW1 = np.dot(dZ2, self.a1.T) / m</span><br><span class="line">    db1 = np.mean(dZ2, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 更新权值</span></span><br><span class="line">    self.W2-=self.lr*dW2</span><br><span class="line">    self.b2-=self.lr*db2</span><br><span class="line">    self.W1 -= self.lr * dW1</span><br><span class="line">    self.b1 -= self.lr * db1</span><br></pre></td></tr></table></figure>
<h2 id="二、数据集解析"><a href="#二、数据集解析" class="headerlink" title="二、数据集解析"></a>二、数据集解析</h2><p>数据集来源：<a href="http://yann.lecun.com/exdb/mnist，选用MNIST手写字数据集训练神经网络，数据集使用Python模块Struct解析二进制文件。手写字特征为28*28=784个像素点，输出为手写字值。解析过程不做详细介绍，其数据解析的一种Python实现为：" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist，选用MNIST手写字数据集训练神经网络，数据集使用Python模块Struct解析二进制文件。手写字特征为28*28=784个像素点，输出为手写字值。解析过程不做详细介绍，其数据解析的一种Python实现为：</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"></span><br><span class="line">file1=<span class="string">"./MNIST_data/train-images.idx3-ubyte"</span></span><br><span class="line">file2=<span class="string">"./MNIST_data/train-labels.idx1-ubyte"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_images_ana</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    <span class="string">"""解析图片数据集 .idx3-ubyte格式"""</span></span><br><span class="line">    <span class="comment"># 以二进制方式读取文件</span></span><br><span class="line">    <span class="keyword">with</span> open(filepath,<span class="string">'rb'</span>) <span class="keyword">as</span> fbj:</span><br><span class="line">        bin_data=fbj.read()</span><br><span class="line">    offset=<span class="number">0</span></span><br><span class="line">    magic_num,image_num,rows_num,column_num=struct.unpack_from(<span class="string">'&gt;iiii'</span>,bin_data,offset)</span><br><span class="line">    offset+=struct.calcsize(<span class="string">'&gt;iiii'</span>)</span><br><span class="line">    imgsize=image_num*rows_num*column_num</span><br><span class="line">    fmt_image=<span class="string">'&gt;'</span>+str(imgsize)+<span class="string">'B'</span>      <span class="comment"># 训练集数据有60000*28*28</span></span><br><span class="line">    images=struct.unpack_from(fmt_image,bin_data,offset)</span><br><span class="line">    img=np.reshape(images,(image_num,rows_num*column_num))     <span class="comment"># 构造一个60000*784的矩阵</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_labels_ana</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    <span class="string">"""解析特征数据集 .idx1-ubyte格式"""</span></span><br><span class="line">    <span class="comment"># 以二进制格式处理文件</span></span><br><span class="line">    <span class="keyword">with</span> open(filepath,<span class="string">'rb'</span>) <span class="keyword">as</span> fbj:</span><br><span class="line">        bin_data=fbj.read()</span><br><span class="line">    offset=<span class="number">0</span></span><br><span class="line">    magic_num,items_num=struct.unpack_from(<span class="string">'&gt;ii'</span>,bin_data,offset)</span><br><span class="line">    offset+=struct.calcsize(<span class="string">'&gt;ii'</span>)</span><br><span class="line">    fmt_label=<span class="string">'&gt;'</span>+str(items_num)+<span class="string">'B'</span></span><br><span class="line">    labels=struct.unpack_from(fmt_label,bin_data,offset)</span><br><span class="line">    label=np.reshape(labels,[items_num])</span><br><span class="line">    <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    imgs=train_images_ana(file1)</span><br><span class="line">    print(np.shape(imgs[<span class="number">1</span>]))</span><br><span class="line">    labels = train_labels_ana(file2)</span><br><span class="line">    print(labels)</span><br><span class="line">    print(np.shape(labels[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 查看前10个手写字灰度图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        img=np.reshape(imgs[i],[<span class="number">28</span>,<span class="number">28</span>])</span><br><span class="line">        plt.imshow(img,cmap=<span class="string">'gray'</span>)</span><br><span class="line">        print(labels[i])</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="三、神经网络搭建"><a href="#三、神经网络搭建" class="headerlink" title="三、神经网络搭建"></a>三、神经网络搭建</h2><p>搭建一个三层神经网络，输入层、隐藏层、输出层节点分别为：784，100，10。</p>
<h2 id="四、附录"><a href="#四、附录" class="headerlink" title="四、附录"></a>四、附录</h2><p>network.py文件，构建神经网络类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> activation_func <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> loss <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,inputnodes,hidnodes,outputnodes,learning_rate)</span>:</span></span><br><span class="line">        self.innodes = inputnodes</span><br><span class="line">        self.hidnodes = hidnodes</span><br><span class="line">        self.outnodes = outputnodes</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        <span class="comment">#各层权重 偏置</span></span><br><span class="line">        self.W1 = np.random.randn(self.hidnodes, self.innodes) * <span class="number">0.01</span></span><br><span class="line">        self.W2 = np.random.randn(self.outnodes, self.hidnodes) * <span class="number">0.01</span></span><br><span class="line">        self.b1 = np.random.randn(self.hidnodes, <span class="number">1</span>) * <span class="number">0.01</span></span><br><span class="line">        self.b2 = np.random.randn(self.outnodes, <span class="number">1</span>) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="comment"># 前向传播算法forward_propagation，实现预测</span></span><br><span class="line">        self.a1 = X.T</span><br><span class="line">        self.z2 = np.dot(self.W1,self.a1)+self.b1</span><br><span class="line">        self.a2 = relu(self.z2)</span><br><span class="line">        self.z3 = np.dot(self.W2,self.a2)+self.b2</span><br><span class="line">        self.a3 = relu(self.z3)</span><br><span class="line">        out = self.a3</span><br><span class="line">        p = np.argmax(out, axis=<span class="number">0</span>)  <span class="comment"># 输出层的最大索引下标即为标签值，标签值0-9</span></span><br><span class="line">        <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line">    <span class="comment">#反向传播：</span></span><br><span class="line">    <span class="comment">#1、m表示样本个数</span></span><br><span class="line">    <span class="comment">#2、梯度下降，更新W和b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dAL)</span>:</span></span><br><span class="line">        m=<span class="number">60000</span></span><br><span class="line">        dZ3=np.multiply(dAL,relu_derivative(self.z3))</span><br><span class="line">        dW2=np.dot(dZ3, self.a2.T)/m</span><br><span class="line">        db2=np.mean(dZ3,axis=<span class="number">1</span>)</span><br><span class="line">        dAL_1 = np.dot(self.W2.T, dZ3)</span><br><span class="line">        dZ2 = np.multiply(dAL_1, relu_derivative(self.z2))</span><br><span class="line">        dW1 = np.dot(dZ2, self.a1.T) / m</span><br><span class="line">        db1 = np.mean(dZ2, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 梯度下降</span></span><br><span class="line">        self.W2-=self.lr*dW2</span><br><span class="line">        self.b2-=self.lr*db2</span><br><span class="line">        self.W1 -= self.lr * dW1</span><br><span class="line">        self.b1 -= self.lr * db1</span><br></pre></td></tr></table></figure>
<p>loss.py文件，损失函数计算代价。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y, y_predict)</span>:</span></span><br><span class="line">    y_predict = np.clip(y_predict,<span class="number">1e-10</span>,<span class="number">1</span><span class="number">-1e-10</span>) <span class="comment">#防止0*log(0)出现。导致计算结果变为NaN</span></span><br><span class="line">    <span class="keyword">return</span> -(y * np.log(y_predict) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - y_predict))</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵损失函数的导函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_der</span><span class="params">(y,y_predict)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -y/y_predict+(<span class="number">1</span>-y)/(<span class="number">1</span>-y_predict)</span><br></pre></td></tr></table></figure>
<p>activation.py文件，激活函数及其导数实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    激活函数</span></span><br><span class="line"><span class="string">    选择非线性的激活函数处理非线性假设</span></span><br><span class="line"><span class="string">    常用激活函数relu、sigmoid</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sig_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#sig函数求导</span></span><br><span class="line">    fx=sigmoid(x)</span><br><span class="line">    <span class="keyword">return</span> fx*(<span class="number">1</span>-fx)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x&gt;=<span class="number">0</span>).astype(np.float64)</span><br></pre></td></tr></table></figure>
<p>load_data.py文件，加载数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"></span><br><span class="line">file1=<span class="string">"./MNIST_data/train-images.idx3-ubyte"</span></span><br><span class="line">file2=<span class="string">"./MNIST_data/train-labels.idx1-ubyte"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_images_ana</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    <span class="string">"""解析图片数据集 .idx3-ubyte格式"""</span></span><br><span class="line">    <span class="comment"># 以二进制方式读取文件</span></span><br><span class="line">    <span class="keyword">with</span> open(filepath,<span class="string">'rb'</span>) <span class="keyword">as</span> fbj:</span><br><span class="line">        bin_data=fbj.read()</span><br><span class="line"></span><br><span class="line">    offset=<span class="number">0</span></span><br><span class="line">    magic_num,image_num,rows_num,column_num=struct.unpack_from(<span class="string">'&gt;iiii'</span>,bin_data,offset)</span><br><span class="line">    offset+=struct.calcsize(<span class="string">'&gt;iiii'</span>)</span><br><span class="line">    imgsize=image_num*rows_num*column_num</span><br><span class="line">    fmt_image=<span class="string">'&gt;'</span>+str(imgsize)+<span class="string">'B'</span>      <span class="comment"># 训练集数据有60000*28*28</span></span><br><span class="line">    images=struct.unpack_from(fmt_image,bin_data,offset)</span><br><span class="line">    img=np.reshape(images,(image_num,rows_num*column_num))     <span class="comment"># 构造一个60000*784的矩阵</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(magic_num,image_num,rows_num,column_num)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_labels_ana</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    <span class="string">"""解析特征数据集 .idx1-ubyte格式"""</span></span><br><span class="line">    <span class="comment"># 以二进制格式处理文件</span></span><br><span class="line">    <span class="keyword">with</span> open(filepath,<span class="string">'rb'</span>) <span class="keyword">as</span> fbj:</span><br><span class="line">        bin_data=fbj.read()</span><br><span class="line"></span><br><span class="line">    offset=<span class="number">0</span></span><br><span class="line">    magic_num,items_num=struct.unpack_from(<span class="string">'&gt;ii'</span>,bin_data,offset)</span><br><span class="line">    offset+=struct.calcsize(<span class="string">'&gt;ii'</span>)</span><br><span class="line">    fmt_label=<span class="string">'&gt;'</span>+str(items_num)+<span class="string">'B'</span></span><br><span class="line">    labels=struct.unpack_from(fmt_label,bin_data,offset)</span><br><span class="line">    label=np.reshape(labels,[items_num])</span><br><span class="line">    <span class="keyword">return</span> label</span><br></pre></td></tr></table></figure>
<p>train.py文件，训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> network <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> load_data <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">input_nodes = <span class="number">784</span></span><br><span class="line">hidden_nodes = <span class="number">100</span></span><br><span class="line">output_nodes = <span class="number">10</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">n = Network(input_nodes, hidden_nodes, output_nodes, learning_rate)</span><br><span class="line"></span><br><span class="line">X=train_images_ana(file1)</span><br><span class="line">Y=train_labels_ana(file2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">60000</span>):</span><br><span class="line">        Y_predict = n.predict(np.mat(X[i]))</span><br><span class="line">        <span class="keyword">if</span> (Y[i]==Y_predict):</span><br><span class="line">            cnt+=<span class="number">1</span></span><br><span class="line">        dA = cross_entropy_der(np.mat(Y[i]),Y_predict)</span><br><span class="line">        n.backward(dA)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'epoch %d:accurac=%f'</span>%(epoch,cnt/<span class="number">60000</span>))</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">wnxy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wnxy.github.io/2020/10/25/Neural-networks-realizes-MNIST-handwriting-recognition/">https://wnxy.github.io/2020/10/25/Neural-networks-realizes-MNIST-handwriting-recognition/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine-Learning</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/MNIST/">MNIST</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/11/02/Data-visualization-matplotlib/"><i class="fa fa-chevron-left">  </i><span>数据可视化模块Matplotlib实操</span></a></div><div class="next-post pull-right"><a href="/2020/10/18/Data-analysis-of-MNIST-handwritten-character-set/"><span>MNIST手写字符集的数据解析</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2022 By wnxy</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><script type="text/javascript" src="/js/crash_cheat.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-koharu"},"display":{"superSample":2,"width":210,"height":420,"position":"right","hOffset":0,"vOffset":-20},"log":false,"tagMode":false});</script><script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var a=0;a<r.length;a++)t=r[a],0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}();var t,e}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body></html>